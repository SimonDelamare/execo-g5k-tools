#!/usr/bin/env python
# Copyright 2015 INRIA Rhone-Alpes, Service Experimentation et
# Developpement
#
# Asclepios is free software: you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Asclepios is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
# or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public
# License for more details, <http://www.gnu.org/licenses/>


import os
import sys
import re
import time
import difflib
import numpy as np
import __main__
from xml.etree import ElementTree as ET
from math import ceil
from getpass import getuser
from string import Template
from execo import logger, TaktukRemote, default_connection_params, sleep, \
    Remote, SequentialActions, SshProcess, configuration, Get
from execo.log import style
from execo_g5k import get_planning, compute_slots, get_resource_attributes, \
    find_max_slot, OarSubmission, oarsub, wait_oar_job_start, deploy, \
    get_cluster_site, Deployment, get_host_site, get_host_attributes, \
    get_host_shortname, oardel, get_g5k_clusters, find_first_slot, g5k_graph, \
    get_cluster_attributes
from execo_g5k.planning import get_job_by_name
from execo_g5k.utils import hosts_list
from execo_engine import copy_outputs
from argparse import ArgumentParser
from execo.process import Process

configuration['color_styles']['OK'] = 'green',  'bold'
configuration['color_styles']['KO'] = 'red', 'bold'
_sys_grep = '| grep "execution time" | awk \'{print $4}\' | cut -d / -f 1'


def main():
    """(A)utomatic (S)imilarity (CL)uster (E)valuation for (P)erformance, (IO)
    and network (S)peed, asclepios, is a tool that allow to check the
    performance homogeneity of a Grid'5000 cluster for cpu, memory, disk,
    network latency and bandwidth. If performs the following actions:
    - retrieve the nodes of the cluster
    - deploy an enviromment and install some bench tools
    - execute various performance test
    - save the results

    See -h for help. Based on execo-2.5."""

    args = init_options()
    try:
        logger.info(style.log_header('Retrieving hosts'))
        job_id, hosts = get_hosts(args.job, args.cluster, args.walltime,
                                      args.now, args.hosts_file)

        logger.info(style.log_header('Hosts configuration'))
        hosts = setup_hosts(hosts,
                            args.env, args.force_deploy, args.no_deploy,
                            args.packages)
        if args.hosts_configuration:
            logger.info(style.log_header('Checking hosts configuration'))
            conf_checks(hosts, args)

        logger.info('%s \n%s', style.log_header('Running benchmarks'),
                    style.emph(args.tests))
        run_benchmarks(hosts, args)
    except KeyboardInterrupt:
        logger.info('Exiting ...')
        pass
    except:
        print "Unexpected error:", sys.exc_info()
        raise
    finally:
        job_id, site = get_job_by_name(args.job,
                                       [get_cluster_site(args.cluster)])
        if args.kill_job:
            logger.info('Destroying job')
            oardel([(job_id, site)])
        else:
            logger.info('Job %s kept alive on %s',  style.emph(job_id),
                        style.host(site))


# Execution functions
def init_options(args=None):
    """Define the options, set log level and create some default values if
    some options are not set"""

    parser = ArgumentParser(description="Reserve all the available nodes on "
                             "a cluster and check that nodes exhibits same "
                             "performance for cpu, disk, network")
    default_tests = 'cpu_mono,cpu_multi,memory,fio_write,' + \
        'lat_gw,bw_frontend,lat_hosts'
    parser.add_argument('-c', '--cluster',
                        help='Name of the cluster')
    mode = parser.add_mutually_exclusive_group()
    mode.add_argument('-v', '--verbose',
                      action="store_true")
    mode.add_argument('-q', '--quiet',
                      action="store_true")
    job = parser.add_mutually_exclusive_group()
    job.add_argument('-j', '--job',
                     help='OAR job number or name')
    job.add_argument('-w', '--walltime',
                     default="1:00:00",
                     help='walltime for the reservation')
    parser.add_argument('-t', '--tests',
                        default=default_tests,
                        help='comma separated list of tests')
    parser.add_argument('-n', '--n-measures',
                        default=1,
                        type=int,
                        help='number of measures to be run for each test')
    parser.add_argument('--kill-job',
                        action="store_true",
                        help='Kill the job at the end')
    parser.add_argument('--now',
                        action="store_true",
                        help='Use the nodes that are available on the cluster')
    parser.add_argument('--full',
                        action="store_true",
                        help='print all the measures')
    parser.add_argument('--env',
                        default="wheezy-x64-base",
                        help='Select environment name, such as '
                            'jessie-x64-base or user:myenv or the path'
                            'to the env file.')
    parser.add_argument('--packages',
                        default="sysbench,fping,iperf,lshw,hwloc,fio",
                        help='List of packages to install')
    parser.add_argument('--hosts-file',
                        help='The path to a file containing the list of hosts')
    parser.add_argument('-o', '--outdir',
                        help='The name of the directory where results will'
                        'be saved')
    parser.add_argument('--force-deploy',
                        action="store_true")
    parser.add_argument('--no-deploy',
                        action="store_true")
    parser.add_argument('--hosts-configuration',
                        action="store_true")

    if not args:
        args = parser.parse_args()

    if args.verbose:
        logger.setLevel('DEBUG')
    elif args.quiet:
        logger.setLevel('WARN')
    else:
        logger.setLevel('INFO')

    if args.cluster not in get_g5k_clusters() and not args.hosts_file:
        logger.error('cluster %s is not a valid g5k cluster, specify it '
                     'with -c or use --hosts-file option',
                     style.emph(args.cluster))
        exit()

    if args.hosts_file:
        args.cluster = 'custom'

    if not args.job:
        args.job = 'asclepios_' + args.cluster

    if not args.outdir:
        args.outdir = args.cluster + '_' + time.strftime("%Y%m%d_%H%M%S_%z")
    if not os.path.exists(args.outdir):
        os.mkdir(args.outdir)
    copy_outputs(args.outdir + '/run_' + args.cluster + '.log',
                 args.outdir + '/run_' + args.cluster + '.log')

    logger.info(style.user3(' ASCLEPIOS \n(A)utomatic (S)imilarity (CL)uster '
            '(E)valuation for (P)erformance, (IO) and network (S)peed'))
    logger.info('%s %s',
               style.host(args.cluster),
               get_cluster_attributes(args.cluster)['model'])

    return args


def get_hosts(job_name, cluster, walltime, now=False, hosts_file=None):
    """Retrieve the job from the job_name, perform a new job if none found
    and return the list of hosts"""
    job_id = None
    if not hosts_file:
        site = get_cluster_site(cluster)
        if job_name.isdigit():
            job_id = int(job_name)
        else:
            job_id, _ = get_job_by_name(job_name, sites=[site])
        if not job_id:
            job_id, site = default_job(job_name, cluster, walltime, now)
            logger.info('Reservation done %s:%s', style.host(site),
                        style.emph(job_id))
        logger.info('Waiting for job start')
        wait_oar_job_start(job_id, site)
        job_info = get_resource_attributes('/sites/' + site +
                                           '/jobs/' + str(job_id))
        hosts = job_info['assigned_nodes']

        hosts.sort(key=lambda h: (h.split('.', 1)[0].split('-')[0],
                                  int(h.split('.', 1)[0].split('-')[1])))
        logger.info('Hosts: %s', hosts_list(hosts))
    else:
        hosts = []
        with open(hosts_file) as f:
            hosts.append(f.readline())

    return job_id, hosts


def setup_hosts(hosts, env, force_deploy, no_deploy, packages=None):
    """Deploy a wheezy-x64-prod environment, configure SSH,
    install some packages and """
    if not no_deploy or force_deploy:
        default_connection_params['user'] = 'root'
        logger.info('Deploying hosts')
        check = 'ls /root' if not force_deploy else False
        num_tries = int(not no_deploy)
        if os.path.exists(env):
            env_file, user, env_name = env, None, None
        elif ':' in env:
            env_file = None
            user, env_name = env.split(':')
        else:
            env_file, user, env_name = None, None, env

        deployed, undeployed = deploy(Deployment(hosts=hosts,
                                                 env_name=env_name,
                                                 env_file=env_file,
                                                 user=user),
                                      num_tries=num_tries,
                                      check_deployed_command=check)
        if len(undeployed) > 0:
            logger.warning('%s have not been deployed',
                           hosts_list(list(undeployed)))
        hosts = list(deployed)
        hosts.sort(key=lambda h: (h.split('.', 1)[0].split('-')[0],
                                  int(h.split('.', 1)[0].split('-')[1])))

        taktuk_conf = ('-s', '-S',
                       '$HOME/.ssh/id_rsa:$HOME/.ssh/id_rsa,' +
                       '$HOME/.ssh/id_rsa.pub:$HOME/.ssh')
        conf_ssh = TaktukRemote('echo "Host *" >> /root/.ssh/config ;' +
                                'echo " StrictHostKeyChecking no" >> ' +
                                '/root/.ssh/config; ',
                                hosts, connection_params={'taktuk_options':
                                                          taktuk_conf}).run()
        if not conf_ssh.ok:
            logger.error('Unable to configure SSH')
            exit()
        if packages:
            logger.info('Installing ' + style.emph(' '.join(packages.split(','))))
            cmd = 'apt-get update && apt-get install -y ' + \
                packages.replace(',', ' ')
            install_pkg = TaktukRemote(cmd, hosts).run()
            if not install_pkg.ok:
                logger.error('Unable to install %s', packages)
                exit()

    return hosts


def conf_checks(hosts, args):
    """ """
    def _group_hosts_by_values(results):
        groups = {}
        for k, v in results.iteritems():
            if v not in groups.keys():
                groups[v] = []
            groups[v].append(k)
        return groups

    def _print_hosts_diff(groups):
        ref = None
        for k in sorted(groups, key=lambda k: len(groups[k]), reverse=True):
            if not ref:
                logger.info('%s (used as reference) %s', style.OK(' OK '),
                            hosts_list(groups[k]))
                ref = k
            else:
                logger.info('%s %s \n%s', style.KO(' KO '),
                            hosts_list(groups[k]), _unidiff_output(ref, k))

    tests = {'kernel': 'Kernel configurations',
             'hardware': 'Detected hardware',
             'hwloc': 'Hardware locality'}
    for t, mes in tests.iteritems():
        logger.info(mes)
        outputs = getattr(__main__, t)(hosts, args.outdir)
        groups = _group_hosts_by_values(outputs)
        if len(groups.keys()) > 1:
            _print_hosts_diff(groups)
            logger.error(style.KO(mes + ' not homogeneous'))
        else:
            logger.info(style.OK(mes + ' homogeneous '))

    logger.info("Retrieving kernel log on each host")
    dmesg(hosts, args.outdir)

    return True


def kernel(hosts, outdir):
    """Retrieve informations about kernel version, loaded module
    and kernel configuration"""

    save_dir = outdir + '/kernel/'
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)
    # Remove this line which are by definition hosts dependent
    ignored_lines = ['kernel.random.entropy_avail', 'kernel.random.boot_id',
                    'kernel.random.uuid', 'kernel.hostname',
                    'kernel.ns_last_pid', 'kernel.pty.nr',
                    '.max_newidle_lb_cost',
                    'kernel.sched_domain.cpu.*.domain1.max_newidle_lb_cost',
                    'fs.inode-nr', 'fs.inode-state',
                    'fs.dentry-state', 'fs.file-nr', 'fs.file-max']
    cmd = 'echo "`uname -srvmpio ; lsmod | sort ; /sbin/sysctl -a | sort`" | ' + \
        ' | '.join(['grep -v ' + pattern for pattern in ignored_lines])
    if len(hosts) > 250:
        logger.warning('TAKTUK MAY SPLIT THE OUTPUT OF sysctl')
        kernel_check = TaktukRemote(cmd, hosts).run()
    else:
        kernel_check = Remote(cmd, hosts).run()
    outputs = {}
    for p in kernel_check.processes:
        outputs[p.host.address] = p.stdout.strip()
        f = open(save_dir + get_host_shortname(p.host.address) + '.conf',
                 'w')
        f.write(p.stdout.strip())
        f.close()

    return outputs


def hardware(hosts, outdir):
    """Use lshw to extract detected hardware, which removal of unique elements
    such as serial numbers.
    """
    save_dir = outdir + '/lshw/'
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)
    shosts = map(get_host_shortname, hosts)
    cmd = 'mkdir -p /tmp/lshw ; lshw -xml > /tmp/lshw/{{shosts}}.xml'
    lshw = TaktukRemote(cmd, hosts).run()
    get_lshw = Get(hosts, ['/tmp/lshw/{{shosts}}.xml'],
                   local_location=save_dir).run()
    if not lshw.ok or not get_lshw.ok:
        return None

    sanitized = {}
    for f in os.listdir(save_dir):
        host = f.split('.')[0]
        xml = ET.parse(outdir + '/lshw/' + f)
        doc = xml.getroot()

        # replace number by X in node hostname
        x = doc.find('.//node[@class=\'system\']')
        if x is not None:
            x.set('id', re.sub('-\d*', '-X', x.get('id')))
        # clear elements that are unique by definition
        to_clear = ['.//node[@class=\'network\']/serial',
                    './/node[@class=\'disk\']/serial',
                    './/node[@class=\'volume\']/serial',
                    './/node[@class=\'system\']/serial',
                    './/node[@class=\'bus\']/serial',
                    './/node[@class=\'input\']/serial',
                    './/node[@class=\'memory\']/serial',
                    './/node[@class=\'processor\']/size']
        for path in to_clear:
            for serial in doc.findall(path):
                serial.clear()
        # empty elements that are unique by definition
        empty = ['.//node[@class=\'network\']/configuration/setting[@id=\'ip\']',
                 './/node[@class=\'disk\']/configuration/setting[@id=\'signature\']',
                 './/node[@class=\'system\']/configuration/setting[@id=\'uuid\']',
                 './/node[@id=\'core\']/slot',
                 './/node[@class=\'volume\']/configuration/setting',
                 './/resource[@type=\'irq\']',
                 './/node[@class=\'processor\']']
        for path in empty:
            for el in doc.findall(path):
                el.set('value', '')

        sanitized[host] = ET.tostring(doc)

    return sanitized


def hwloc(hosts, outdir):
    """Use hwloc to """
    save_dir = outdir + '/hwloc/'
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)
    shosts = map(get_host_shortname, hosts)
    cmd = 'mkdir -p /tmp/hwloc ; hwloc-ls --of xml > /tmp/hwloc/{{shosts}}.xml'
    hwloc = TaktukRemote(cmd, hosts).run()
    get_hwloc = Get(hosts, ['/tmp/hwloc/{{shosts}}.xml'],
                   local_location=save_dir).run()
    if not hwloc.ok or not get_hwloc.ok:
        return None

    sanitized = {}
    for f in os.listdir(save_dir):
        host = f.split('.')[0]
        xml = ET.parse(save_dir + f)
        doc = xml.getroot()
        clear_paths = ['.//info[@name=\'HostName\']',
                       './/info[@name=\'Address\']',
                       './/info[@name=\'DMIProductUUID\']',
                       './/info[@name=\'DMIProductSerial\']',
                       './/info[@name=\'DMIBoardSerial\']',
                       './/info[@name=\'DMIChassisSerial\']']
        for path in clear_paths:
            for serial in doc.findall(path):
                serial.clear()

        sanitized[host] = ET.tostring(doc)
    return sanitized


def _unidiff_output(expected, actual):
    """
    Helper function. Returns a string containing the unified diff of two
    multiline strings.
    """
    expected = expected.splitlines(1)
    actual = actual.splitlines(1)
    diff = difflib.unified_diff(expected, actual)

    return ''.join(diff)


def dmesg(hosts, outdir):
    """Retrieve the content of kernel messages"""
    Process("mkdir " + outdir + "/dmesg/").run()
    dmesg = TaktukRemote("dmesg", hosts)
    for p in dmesg.processes:
        p.stdout_handlers.append(outdir + "/dmesg/%s.out" % (p.host.address,))
    dmesg.run()
    logger.info("dmesg output stored in " + outdir + "/dmesg")


# Benchmarking methods functions
def run_benchmarks(hosts, args):
    """ """
    for test in args.tests.split(','):
        logger.info(style.user3('Starting %s test' % (test.upper())))
        for i in range(args.n_measures):
            if default_connection_params['user'] == 'root':
                clear_cache(hosts)
            logger.info(style.emph('%s/%s' % (i + 1, args.n_measures)))
            bench_results = getattr(__main__, test)(hosts)
            print_bench_result(test, bench_results, args.full)
            save_bench_results(test, bench_results, args.outdir)
        logger.info(style.user3(test.upper() + ' test done') + '\n')


def cpu_mono(hosts, max_prime=10000):
    """Execute a stress intensive bench using one core"""
    cmd = 'sysbench --test=cpu --cpu-max-prime=%s run %s' % \
        (max_prime, _sys_grep)
    logger.info('Launching CPU_MONO benchmark with \n%s', style.command(cmd))
    monocore = TaktukRemote(cmd, hosts).run()

    results = parse_hosts_perf(monocore)

    return results


def cpu_multi(hosts, max_prime=100000):
    """Execute a stress intensive using all the core of the machine"""
    n_core = get_host_attributes(hosts[0])['architecture']['nb_cores']
    cmd = 'sysbench --num-threads=%s --test=cpu --cpu-max-prime=%s run %s' % \
        (n_core, max_prime, _sys_grep)
    logger.info('Launching CPU_MULTI benchmark with \n%s', style.command(cmd))
    multicore = TaktukRemote(cmd, hosts).run()

    results = parse_hosts_perf(multicore)

    return results


def memory(hosts):
    """Execute a memory intensive test that use the whole memory of the node"""
    n_core = get_host_attributes(hosts[0])['architecture']['nb_cores']
    mem_size = int(ceil(get_host_attributes(hosts[0])['main_memory']['ram_size']
                        * 0.95))
    # mem_size must be a multiple of 4 in sysbench
    mem_size = mem_size - mem_size % 4
    cmd = 'sysbench --test=memory --num-threads=%s --memory-block-size=%s ' \
        'run %s' % (n_core, mem_size, _sys_grep)
    logger.info('Launching MEM benchmark with \n%s', style.command(cmd))
    mem_test = TaktukRemote(cmd, hosts).run()

    results = parse_hosts_perf(mem_test)

    return results


def fio_write(hosts):
    """ """
    cmd = "fio --ioengine=libaio --direct=1 --bs=4m --size=1g " + \
        "--directory=/tmp --iodepth=32 --name=file1 --rw=write |" + \
        "grep aggrb |  awk \'{print $3}\' | cut -d '=' -f 2"
    logger.info('Launching WRITE benchmark with \n%s', style.command(cmd))
    write_test = TaktukRemote(cmd, hosts).run()

    for p in write_test.processes:
        non_decimal = re.compile(r'[^\d.]+')
        p.stdout = non_decimal.sub('', p.stdout)

    results = parse_hosts_perf(write_test)

    return results


def fio_read(hosts):
    """ """
    cmd = "fio --ioengine=libaio --direct=1 --bs=4m --size=1g " + \
        "--directory=/tmp --iodepth=32 --name=file1 --rw=read |" + \
        "grep aggrb |  awk \'{print $3}\' | cut -d '=' -f 2"
    logger.info('Launching READ benchmark with \n%s', style.command(cmd))
    write_test = TaktukRemote(cmd, hosts).run()

    for p in write_test.processes:
        non_decimal = re.compile(r'[^\d.]+')
        p.stdout = non_decimal.sub('', p.stdout)

    results = parse_hosts_perf(write_test)

    return results


def sysbench_fio(hosts):
    """Execute sequential read write """
    attr = get_host_attributes(hosts[0])
    n_core = attr['architecture']['nb_cores']
    perf = float(attr['performance']['node_flops'])
    filesize = int(ceil(float(perf) / 2. / 10 ** 9))

    if filesize == 0:
        logger.warning('No performance information in Reference API for %s',
                       get_host_shortname(hosts[0]))
        filesize = 10
    cmd = Template("cd /tmp && sysbench --num-threads=%s --test=fileio "
                   "--file-total-size=%sG --file-test-mode=seqwr "
                   "$action $grep" % (n_core, filesize))
    logger.info('Preparing FIO benchmark (%s Gb file)', filesize)
    prepare = TaktukRemote(cmd.substitute(action='prepare', grep=""),
                           hosts).run()
    if not prepare.ok:
        logger.error('Unable to prepare the data for FIO benchmark\n%s',
                     '\n'.join([p.host.address + ': ' + p.stdout.strip()
                                for p in prepare.processes]))
        return {}
    logger.info('Launching FIO benchmark with \n%s',
                style.command(cmd.substitute(action='run', grep=_sys_grep)))
    run = TaktukRemote(cmd.substitute(action='run', grep=_sys_grep),
                       hosts).run()
    logger.info('Cleaning FIO benchmark')
    clean = TaktukRemote(cmd.substitute(action='cleanup', grep=""),
                         hosts).run()
    if not clean.ok:
        logger.error('Unable to clean the data for FIO benchmark\n%s',
                     '\n'.join([p.host.address + ': ' + p.stdout.strip()
                                for p in clean.processes]))
        return {}

    results = parse_hosts_perf(run)

    return results


def lat_gw(hosts, n_ping=10):
    """Measure the latency between hosts and site router"""
    cmd = 'ping -c %s gw-%s |tail -1| awk \'{print $4}\' |cut -d \'/\' -f 2' \
        % (n_ping, get_host_site(hosts[0]))
    logger.info('Executing ping from hosts to site router \n%s',
                style.command(cmd))
    ping_gw = TaktukRemote(cmd, hosts).run()

    results = {}
    for p in ping_gw.processes:
        link = get_host_shortname(p.host.address).split('-')[1] + '->' + \
            p.remote_cmd.split('|')[0].split()[3].strip()
        results[link] = float(p.stdout.strip())

    return results


def lat_hosts(hosts, n_ping=10):
    """Measure latency between hosts using fping"""
    cmd = 'fping -c %s -e -q %s 2>&1 | awk \'{print $1" "$8}\'' % \
        (n_ping, ' '.join([get_host_shortname(h) for h in hosts]))
    logger.info('Executing fping from hosts to all other hosts \n%s',
                style.command(cmd))
    fping = TaktukRemote(cmd, hosts).run()

    results = {}
    for p in fping.processes:
        src = get_host_shortname(p.host.address).split('-')[1]
        for h_res in p.stdout.strip().split('\n'):
            h, tmpres = h_res.split()
            dst = get_host_shortname(h).split('-')[1]
            res = tmpres.split('/')[1]
            if src != dst:
                results[src + '->' + dst] = float(res)

    return results


def bw_frontend(hosts):
    """Sequential measurement of bandwidth between hosts and frontend"""
    frontend = get_host_site(hosts[0])
    f_user = getuser()
    port = '4567'
    with SshProcess('iperf -s -p ' + port, frontend,
                    connection_params={"user": f_user}).start() as iperf_serv:
        iperf_serv.expect("^Server listening", timeout=10)
        logger.info('IPERF server running on %s, launching measurement',
                    style.host(frontend))
        actions = [Remote('iperf -f m -t 5 -c ' + frontend + ' -p ' + port +
                         ' | tail -1| awk \'{print $8}\'', [h]) for h in hosts]
        iperf_clients = SequentialActions(actions).run()
    iperf_serv.wait()

    results = {}
    for p in iperf_clients.processes:
        link = get_host_shortname(p.host.address).split('-')[1] + '->' + \
            frontend
        results[link] = float(p.stdout.strip())

    return results


def bw_oneone(hosts):
    """Parallel measurements of bandwitdh from one host to another"""
    servers = hosts
    clients = [hosts[-1]] + hosts[0:-1]
    logger.info('Launching iperf measurements')
    with TaktukRemote('iperf -s', servers).start() as iperf_serv:
        iperf_serv.expect("^Server listening")
        logger.info('IPERF servers are running, launching measurement')
        iperf_clients = TaktukRemote('iperf -f m -t 30 -c {{servers}}'
                                     '| tail -1 | awk \'{print $7}\'',
                                     clients).run()
    iperf_serv.wait()
    results = {}
    for p in iperf_clients.processes:
        src = get_host_shortname(p.host.address).split('-')[1]
        dst = get_host_shortname(p.remote_cmd.split('|')[0].split()[6]\
                                 .strip()).split('-')[1]
        link = src + '->' + dst
        results[link] = float(p.stdout.strip())

    return results


def bw_hosts(hosts):
    """Sequential measurements of bandwidth from all hosts to all others"""
    results = {}
    g = g5k_graph(hosts)
    with TaktukRemote('iperf -s', hosts).start() as iperf_serv:
        iperf_serv.expect("^Server listening", timeout=10)
        for src in hosts:
            logger.info('%s to others',
                        style.host(get_host_shortname(src)))
            dests = g.get_host_neighbours(get_host_shortname(src))
            actions = [Remote('iperf -f m -t 5 -c ' + dst +
                              ' | tail -1 | awk \'{print $8}\'', src)
                       for dst in dests]
            iperf_clients = SequentialActions(actions).run()
            for p in iperf_clients.processes:
                link = src.split('-')[1] + '->' + \
                    p.remote_cmd.split('|')[0].split()[6].strip()
                results[link] = float(p.stdout.strip())

    iperf_serv.wait()

    return results


# Various function
def print_bench_result(name, results, full=False):
    """ """
    name = name.upper()
    mean, median, stdev = compute_stats(results)

    logger.info('Stats: '
                + '\n' + style.emph('mean'.ljust(10)) + str(mean)
                + '\n' + style.emph('median'.ljust(10)) + str(median)
                + '\n' + style.emph('stdev'.ljust(10)) + str(stdev))
    error = []
    warning = []
    if '->' not in results.keys()[0]:
        sort_func = lambda h: int(h.split('-')[1])
    else:
        sort_func = lambda h: int(h.split('->')[0])

    for h in sorted(results.keys(),
                    key=sort_func):
        res = results[h]

        if res > (median + 2 * stdev) \
            or res < (median - 2 * stdev):
            if abs((res - median) / median) < 0.10:
                warning.append(h)
                logger.warning('%s %s %s', name, style.host(h).ljust(15),
                               style.report_warn(res))
            else:
                error.append(h)
                logger.error('%s %s %s', name, style.host(h).ljust(15),
                             style.report_error(res))
        elif full:
            logger.info('%s %s %s', name, style.host(h).ljust(15),
                               res)

    if len(error) > 0:
        logger.info('Need to open a bug ?')
    elif len(warning) > 0:
        logger.warning('%s performance is slightly not homogeneous ?',
                       name.upper())
    else:
        logger.info('%s performance is homogeneous', name.upper())


def save_bench_results(test, results, outdir):
    """Save results """
    base_fname = outdir + '/bench_' + test
    i = 0
    fname = base_fname + '.' + str(i)
    while os.path.exists(fname):
        fname = base_fname + '.' + str(i)
        i += 1
    f = open(fname, 'w')
    f.write('\n'.join([e + '\t' + str(val) for e, val in results.iteritems()]))
    f.close()
    logger.info('Results saved in %s', fname)


def compute_stats(results):
    """ """
    mean = np.mean(np.array(results.values()))
    median = np.median(np.array(results.values()))
    stdev = np.std(np.array(results.values()))

    return mean, median, stdev


def parse_hosts_perf(act):
    """ """
    results = {get_host_shortname(p.host.address): float(p.stdout.strip())
                         for p in act.processes}

    return results


def clear_cache(hosts):
    """ """
    clear = TaktukRemote('sync; echo 3 > '
                         '/proc/sys/vm/drop_caches ;'
                         'umount /tmp; sleep 5; mount /tmp',
                         hosts).run()
    sleep(2)
    return clear.ok


def default_job(job_name, cluster, walltime, now=False):
    """ """
    logger.info('No job running, making a reservation')
    wanted = {cluster: 0}
    planning = get_planning(wanted.keys())
    slots = compute_slots(planning, walltime)
    if now:
        start_date, _, resources = find_first_slot(slots, wanted)
    else:
        start_date, _, resources = find_max_slot(slots, wanted)
    jobs_specs = [(OarSubmission(resources='{cluster=\'%s\'}/nodes=%s'
                                 % (cluster, resources[cluster]),
                                 job_type="deploy",
                                 walltime=walltime,
                                 reservation_date=start_date,
                                 name=job_name),
                   get_cluster_site(cluster))]
    return oarsub(jobs_specs)[0]


if __name__ == "__main__":
    main()
